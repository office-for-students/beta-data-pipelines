# DevOps Pipeline definition for the OFS Beta Backend Data Pipeline 

#Disable CI
trigger: none

#Disable PR
pr: none

# Build Agent Resources
pool:
  vmImage: 'windows-latest'
strategy:
  matrix:
    Python3:
      python.version: '3.11'

# Build Process Tree
steps:

# Enforce Python Version (see above)
- task: UsePythonVersion@0
  inputs:
    versionSpec: '$(python.version)'
  displayName: 'Use Python $(python.version)'


- task: PythonScript@0
  displayName: 'Download and Process HESA Data'
  inputs:
    scriptSource: 'inline'
    script: |
      """Download the HESA gzip file, extract the kis*.xml file, and prepare it for storage"""
      import urllib.request
      import time
      import gzip
      import shutil
      import os
      import glob
      import tempfile

      # Set up request headers
      hdr = {
          'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',
          'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
          'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',
          'Accept-Encoding': 'none',
          'Accept-Language': 'en-US,en;q=0.8',
          'Connection': 'keep-alive'
      }

      # Download the file with retry logic
      url = '$(HesaURL)'
      max_retries = 3
      retry_delay = 10
      attempt = 0
      temp_dir = r'$(Agent.TempDirectory)'
      gzip_file_path = os.path.join(temp_dir, 'hesa_data.gz')
      extract_dir = os.path.join(temp_dir, 'extracted_data')

      # Create extraction directory if it doesn't exist
      if not os.path.exists(extract_dir):
          os.makedirs(extract_dir)

      # Download the gzip file
      while attempt < max_retries:
          try:
              print(f'Attempting to fetch file from {url}... Attempt #{attempt + 1}')

              hesa_request = urllib.request.Request(url, headers=hdr)
              hesa_response = urllib.request.urlopen(hesa_request)

              print('Beginning file download...')
              with open(gzip_file_path, 'wb') as f:
                  while True:
                      data = hesa_response.read(1024)
                      if not data:
                          break
                      f.write(data)

              print('Download complete!')
              break
          except Exception as e:
              print(f'Error during download attempt {attempt + 1}: {e}')
              attempt += 1
              if attempt < max_retries:
                  print(f'Retrying in {retry_delay} seconds...')
                  time.sleep(retry_delay)
              else:
                  print('Max retries reached. Failing the request.')
                  raise

      # Extract the gzip file
      print('Extracting gzip file...')
      try:
          with gzip.open(gzip_file_path, 'rb') as f_in:
              # Extract to a temporary directory
              with open(os.path.join(extract_dir, 'extracted_content'), 'wb') as f_out:
                  shutil.copyfileobj(f_in, f_out)
          print('Extraction complete!')
      except Exception as e:
          print(f'Error extracting gzip file: {e}')
          raise

      # Find the kis*.xml file
      print('Looking for kis*.xml file...')
      kis_file_path = None

      # Search for kis*.xml in the extracted directory and subdirectories
      for root, dirs, files in os.walk(extract_dir):
          for file in files:
              if file.startswith('kis') and file.endswith('.xml'):
                  kis_file_path = os.path.join(root, file)
                  print(f'Found matching file: {kis_file_path}')
                  break
          if kis_file_path:
              break

      if not kis_file_path:
          raise Exception('No kis*.xml file found in the extracted data')

      # Create the gzipped output file
      output_file = os.path.join(temp_dir, 'latest.xml.gz')
      print(f'Creating gzipped output file: {output_file}')

      with open(kis_file_path, 'rb') as f_in:
          with gzip.open(output_file, 'wb') as f_out:
              shutil.copyfileobj(f_in, f_out)

      print('Processing complete! File ready for upload to blob storage.')


- task: AzureFileCopy@3
  displayName: 'Copy kis archive to storage blob'
  inputs:
    SourcePath: '$(Agent.TempDirectory)\latest.xml.gz'
    azureSubscription: '$(AzureSubscription)'
    Destination: 'AzureBlob'
    storage: '$(AzureStorageAccount)'
    ContainerName: '$(AzureStorageBlob)'


- task: PythonScript@0
  displayName: 'Start Data Import Pipeline'
  inputs:
    scriptSource: 'inline'
    script: |
      """Send a request to the Data Import Pipeline in order to trigger it"""
      import urllib.request

      hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',
      'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
      'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',
      'Accept-Encoding': 'none',
      'Accept-Language': 'en-US,en;q=0.8',
      'Connection': 'keep-alive'}

      url = '$(DataPipelineURL)'

      print('Sending request to pipeline...')

      pipeline_request = urllib.request.Request(url, headers=hdr)
      pipeline_response = urllib.request.urlopen(pipeline_request, timeout=120)

      print('Request sent!')